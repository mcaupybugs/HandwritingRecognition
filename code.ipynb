{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitbasecondae0f52b00d66e4771b9f6f763f4d6dfb8",
   "display_name": "Python 3.6.10 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/written_name_train_v2.csv')\n",
    "valid = pd.read_csv('./data/written_name_validation_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;bound method NDFrame.head of                 FILENAME       IDENTITY\n0        TRAIN_00001.jpg      BALTHAZAR\n1        TRAIN_00002.jpg          SIMON\n2        TRAIN_00003.jpg          BENES\n3        TRAIN_00004.jpg        LA LOVE\n4        TRAIN_00005.jpg         DAPHNE\n...                  ...            ...\n330956  TRAIN_330957.jpg          LENNY\n330957  TRAIN_330958.jpg        TIFFANY\n330958  TRAIN_330959.jpg  COUTINHO DESA\n330959  TRAIN_330960.jpg         MOURAD\n330960  TRAIN_330961.jpg        HELOISE\n\n[330961 rows x 2 columns]&gt;"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "train.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of train NaN: 565\nNumber of valid NaN: 78\n"
    }
   ],
   "source": [
    "#Cleaning the data \n",
    "print(\"Number of train NaN:\",train['IDENTITY'].isnull().sum())\n",
    "print(\"Number of valid NaN:\",valid['IDENTITY'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping NaN Values \n",
    "train.dropna(axis=0,inplace=True)\n",
    "valid.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for unreadable images \n",
    "unreadable = train[train['IDENTITY'] == 'UNREADABLE']\n",
    "unreadable.reset_index(inplace = True, drop=True)\n",
    "train = train[train['IDENTITY'] != 'UNREADABLE']\n",
    "valid = valid[valid['IDENTITY'] != 'UNREADABLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the lowercase to uppercase to maintain uniformity\n",
    "train['IDENTITY']=train['IDENTITY'].str.upper()\n",
    "valid['IDENTITY']=valid['IDENTITY'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reseting the index of elements\n",
    "train.reset_index(inplace=True,drop=True)\n",
    "valid.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPROCESSING AND PREPARING THE IMAGES\n",
    "\n",
    "#Image loaded as grayscale and reshaped to 256 and 64\n",
    "#The width and height are cropped if they are greater than 256 and 64 respectively. If they are smaller, then the image is padded with white pixels. Finally the image is rotated clockwise to bring the image shape to (x, y).\n",
    "\n",
    "\n",
    "#The image is then normalized to range [0, 1]\n",
    "\n",
    "def preprocess(img):\n",
    "    (h,w)=img.shape\n",
    "    final_img = np.ones([64,256])*255 \n",
    "\n",
    "    if w>256:\n",
    "        img=img[:,:256]\n",
    "    \n",
    "    if h>64:\n",
    "        img = img[:64,:]\n",
    "\n",
    "    final_img[:h,:w] = img\n",
    "    return cv2.rotate(final_img,cv2.ROTATE_90_CLOCKWISE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Max size is 30000 train size \n",
    "#Max valid size is 3000 \n",
    "train_size = 1000\n",
    "valid_size=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[]\n",
    "for i in range(train_size):\n",
    "    img_dir = './data/train_v2/train/'+train.loc[i,'FILENAME']\n",
    "    image = cv2.imread(img_dir,cv2.IMREAD_GRAYSCALE)\n",
    "    image = preprocess(image)\n",
    "    image = image/255\n",
    "    train_x.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = []\n",
    "for i in range(valid_size):\n",
    "        img_dir = './data/validation_v2/validation/'+valid.loc[i,'FILENAME']\n",
    "        image = cv2.imread(img_dir,cv2.IMREAD_GRAYSCALE)\n",
    "        image = preprocess(image)\n",
    "        image = image/255\n",
    "        valid_x.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping the array\n",
    "train_x = np.array(train_x).reshape(-1, 256, 64, 1)\n",
    "valid_x = np.array(valid_x).reshape(-1, 256, 64, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets = u\"ABCDEFGHIJKLMNOPQRSTUVWXYZ-'\"\n",
    "max_str_len = 24 #max length of the input labels\n",
    "num_of_characters = len(alphabets)+1 #+1 for blank\n",
    "num_of_timestamps = 64 #max length of the predicted labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_num(label):\n",
    "    label_num=[]\n",
    "    for ch in label:\n",
    "        label_num.append(alphabets.find(ch))\n",
    "    return np.array(label_num)\n",
    "\n",
    "def num_to_label(num):\n",
    "    ret = \"\"\n",
    "    for ch in num:\n",
    "        if ch == -1:\n",
    "            break\n",
    "        else:\n",
    "            ret+=alphabets[ch]\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "VISHAL \n [21  8 18  7  0 11]\n"
    }
   ],
   "source": [
    "name = 'VISHAL'\n",
    "print(name,'\\n',label_to_num(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.ones([train_size,max_str_len])*-1\n",
    "train_label_len = np.zeros([train_size,1])\n",
    "train_input_len = np.ones([train_size,1])*(num_of_timestamps-2)\n",
    "train_output = np.zeros([train_size])\n",
    "\n",
    "for i in range(train_size):\n",
    "    train_label_len[i] = len(train.loc[i,'IDENTITY'])\n",
    "    train_y[i,0:len(train.loc[i,'IDENTITY'])] = label_to_num(train.loc[i,'IDENTITY'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_y = np.ones([valid_size, max_str_len]) * -1\n",
    "valid_label_len = np.zeros([valid_size, 1])\n",
    "valid_input_len = np.ones([valid_size, 1]) * (num_of_timestamps-2)\n",
    "valid_output = np.zeros([valid_size])\n",
    "\n",
    "for i in range(valid_size):\n",
    "    valid_label_len[i] = len(valid.loc[i, 'IDENTITY'])\n",
    "    valid_y[i, 0:len(valid.loc[i, 'IDENTITY'])]= label_to_num(valid.loc[i, 'IDENTITY'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "True label :  NOUR \ntrain_y :  [13. 14. 20. 17. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n -1. -1. -1. -1. -1. -1.] \ntrain_label_len :  [4.] \ntrain_input_len :  [62.]\n"
    }
   ],
   "source": [
    "print('True label : ',train.loc[100, 'IDENTITY'] , '\\ntrain_y : ',train_y[100],'\\ntrain_label_len : ',train_label_len[100], \n",
    "      '\\ntrain_input_len : ', train_input_len[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input,Conv2D,MaxPooling2D,Reshape,Bidirectional,LSTM,Dense,Lambda,Activation,BatchNormalization,Dropout\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "object of type &#39;Dense&#39; has no len()",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m&lt;ipython-input-30-679fcb568ca6&gt;\u001b[0m in \u001b[0;36m&lt;module&gt;\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m&#39;softmax&#39;\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m&#39;softmax&#39;\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---&gt; 32\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn(&#39;Update your `&#39; + object_name + &#39;` call to the &#39; +\n\u001b[1;32m     90\u001b[0m                               &#39;Keras 2 API: &#39; + signature, stacklevel=2)\n\u001b[0;32m---&gt; 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 &#39;inputs&#39; in kwargs and &#39;outputs&#39; in kwargs):\n\u001b[1;32m     93\u001b[0m             \u001b[0;31m# Graph network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---&gt; 94\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# Keep track of the network&#39;s nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         nodes, nodes_by_depth, layers, layers_by_depth = _map_graph_network(\n\u001b[0;32m--&gt; 241\u001b[0;31m             self.inputs, self.outputs)\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m   1432\u001b[0m                   \u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m                   \u001b[0mnode_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-&gt; 1434\u001b[0;31m                   tensor_index=tensor_index)\n\u001b[0m\u001b[1;32m   1435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes_in_decreasing_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m         \u001b[0;31m# Propagate to all previous tensors connected to this node.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-&gt; 1415\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1416\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type &#39;Dense&#39; has no len()"
     ]
    }
   ],
   "source": [
    "input_data = Input(shape=(256,64,1),name='input')\n",
    "\n",
    "inner = Conv2D(32, (3, 3), padding='same', name='conv1', kernel_initializer='he_normal')(input_data)  \n",
    "inner = BatchNormalization()(inner)\n",
    "inner = Activation('relu')(inner)\n",
    "inner = MaxPooling2D(pool_size=(2, 2), name='max1')(inner)\n",
    "\n",
    "inner = Conv2D(64, (3, 3), padding='same', name='conv2', kernel_initializer='he_normal')(inner)\n",
    "inner = BatchNormalization()(inner)\n",
    "inner = Activation('relu')(inner)\n",
    "inner = MaxPooling2D(pool_size=(2, 2), name='max2')(inner)\n",
    "inner = Dropout(0.3)(inner)\n",
    "\n",
    "inner = Conv2D(128, (3, 3), padding='same', name='conv3', kernel_initializer='he_normal')(inner)\n",
    "inner = BatchNormalization()(inner)\n",
    "inner = Activation('relu')(inner)\n",
    "inner = MaxPooling2D(pool_size=(1, 2), name='max3')(inner)\n",
    "inner = Dropout(0.3)(inner)\n",
    "\n",
    "# CNN to RNN\n",
    "inner = Reshape(target_shape=((64, 1024)), name='reshape')(inner)\n",
    "inner = Dense(64, activation='relu', kernel_initializer='he_normal', name='dense1')(inner)\n",
    "\n",
    "## RNN\n",
    "inner = Bidirectional(LSTM(256, return_sequences=True), name = 'lstm1')(inner)\n",
    "inner = Bidirectional(LSTM(256, return_sequences=True), name = 'lstm2')(inner)\n",
    "\n",
    "## OUTPUT\n",
    "inner = Dense(num_of_characters, kernel_initializer='he_normal',name='dense2')(inner)\n",
    "y_pred = Activation('softmax', name='softmax')(inner)\n",
    "\n",
    "model = Model(inputs=input_data, outputs=y_pred)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the ctc loss function\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    # the 2 is critical here since the first couple outputs of the RNN\n",
    "    # tend to be garbage\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = Input(name='gtruth_labels', shape=[max_str_len], dtype='float32')\n",
    "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "\n",
    "ctc_loss = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])\n",
    "model_final = Model(inputs=[input_data, labels, input_length, label_length], outputs=ctc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss calculation occurs elsewhere, so we use a dummy lambda function for the loss\n",
    "model_final.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=Adam(lr = 0.0001))\n",
    "\n",
    "model_final.fit(x=[train_x, train_y, train_input_len, train_label_len], y=train_output, \n",
    "                validation_data=([valid_x, valid_y, valid_input_len, valid_label_len], valid_output),\n",
    "                epochs=60, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(valid_x)\n",
    "decoded = K.get_value(K.ctc_decode(preds, input_length=np.ones(preds.shape[0])*preds.shape[1], \n",
    "                                   greedy=True)[0][0])\n",
    "\n",
    "prediction = []\n",
    "for i in range(valid_size):\n",
    "    prediction.append(num_to_label(decoded[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = valid.loc[0:valid_size, 'IDENTITY']\n",
    "correct_char = 0\n",
    "total_char = 0\n",
    "correct = 0\n",
    "\n",
    "for i in range(valid_size):\n",
    "    pr = prediction[i]\n",
    "    tr = y_true[i]\n",
    "    total_char += len(tr)\n",
    "    \n",
    "    for j in range(min(len(tr), len(pr))):\n",
    "        if tr[j] == pr[j]:\n",
    "            correct_char += 1\n",
    "            \n",
    "    if pr == tr :\n",
    "        correct += 1 \n",
    "    \n",
    "print('Correct characters predicted : %.2f%%' %(correct_char*100/total_char))\n",
    "print('Correct words predicted      : %.2f%%' %(correct*100/valid_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/handwriting-recognition/written_name_test_v2.csv')\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i in range(6):\n",
    "    ax = plt.subplot(2, 3, i+1)\n",
    "    img_dir = '/kaggle/input/handwriting-recognition/test_v2/test/'+test.loc[i, 'FILENAME']\n",
    "    image = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    \n",
    "    image = preprocess(image)\n",
    "    image = image/255.\n",
    "    pred = model.predict(image.reshape(1, 256, 64, 1))\n",
    "    decoded = K.get_value(K.ctc_decode(pred, input_length=np.ones(pred.shape[0])*pred.shape[1], \n",
    "                                       greedy=True)[0][0])\n",
    "    plt.title(num_to_label(decoded[0]), fontsize=12)\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.subplots_adjust(wspace=0.2, hspace=-0.8)"
   ]
  }
 ]
}